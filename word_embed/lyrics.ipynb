{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T05:11:54.779860Z",
     "start_time": "2020-05-15T05:11:16.446695Z"
    }
   },
   "outputs": [],
   "source": [
    "## Word2Vec on direct translated lyrics\n",
    "## https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt\n",
    "## https://datascience.stackexchange.com/questions/23969/sentence-similarity-prediction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import jieba\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "#from gensim.models import KeyedVectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T05:32:24.704844Z",
     "start_time": "2020-05-15T05:32:24.701201Z"
    }
   },
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:17:30.732694Z",
     "start_time": "2020-05-15T06:17:30.727002Z"
    }
   },
   "outputs": [],
   "source": [
    "en_file = 'shinpakusuu_en.txt'\n",
    "zh_file = 'shinpakusuu_zh.txt'\n",
    "\n",
    "def read_f(x_file):\n",
    "    with open(x_file,'r', encoding='utf-8') as file:\n",
    "        x_list = file.read().splitlines()\n",
    "        \n",
    "    return list(filter(None, x_list))\n",
    "    \n",
    "en_list = read_f(en_file)\n",
    "        \n",
    "zh_list = read_f(zh_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:28:47.528046Z",
     "start_time": "2020-05-15T06:28:47.505876Z"
    }
   },
   "outputs": [],
   "source": [
    "# English\n",
    "# 1. Tokenize Sentence -> Words\n",
    "# 2. Remove punctuation and stopwords\n",
    "# 3. Stemming Words\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize  # splits by contractions which I don't like\n",
    "from string import punctuation\n",
    "\n",
    "en_stopwords=stopwords.words(\"english\")\n",
    "stemmer=PorterStemmer()\n",
    "punctuation = punctuation +'–’“”'\n",
    "\n",
    "en_list_token = [word_tokenize(s) for s in en_list]\n",
    "en_list_proc = []\n",
    "for s in en_list_token:\n",
    "    en_list_proc.append([stemmer.stem(w.lower()) for w in s if w.lower() not in en_stopwords and w not in punctuation])\n",
    "\n",
    "# Chinese\n",
    "# 1. Segmentation\n",
    "# 2. Remove punctuation and stopwords\n",
    "\n",
    "import re\n",
    "with open('zh_stopwords.txt','r', encoding='utf-8') as file:\n",
    "    zh_stopwords = file.read()\n",
    "zh_stopwords = re.sub('[ A-Za-z]+\\n', ',', zh_stopwords)\n",
    "zh_stopwords = zh_stopwords.translate(str.maketrans('', '', '\\n')).split(',') \n",
    "zh_stopwords = list(filter(None, zh_stopwords))\n",
    "punctuation = punctuation + '，「」。！？《》【】、'\n",
    "\n",
    "zh_list_proc = []\n",
    "for s in zh_list:\n",
    "    zh_list_proc.append([w for w in jieba.cut(s) if w not in zh_stopwords and w not in punctuation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:34:40.222474Z",
     "start_time": "2020-05-15T06:34:40.214531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['心脏', '停下'],\n",
       " ['一定', '觉得', '已经', '充分', '享受', '世界', '结束'],\n",
       " ['彷佛', '没', '做', '完', '事', '几乎', '没有', '般'],\n",
       " ['希望', '身旁', '一直', '笑'],\n",
       " ['仍然', '想', '这颗', '心', '跳动', '时间', '守护'],\n",
       " ['那件事', '生存', '意义'],\n",
       " ['一个', '一个', '数着', '相同', '眼泪'],\n",
       " ['再度', '了解', '彼此'],\n",
       " ['巨大', '跳动', '声', '传达'],\n",
       " ['重叠', '声响', '流泄', '思念'],\n",
       " ['约定', '再也', '不要', '分开'],\n",
       " ['希望', '不要', '寂寞'],\n",
       " ['心脏', '一分钟'],\n",
       " ['会', '喊', '出', '70', '次', '正', '活着'],\n",
       " ['一起', '时', '会', '稍微', '加快脚步'],\n",
       " ['喊出', '110', '次', '我爱你'],\n",
       " ['仍然', '想', '这颗', '心', '跳动', '时间', '守护'],\n",
       " ['那件事', '生存', '意义'],\n",
       " ['一次', '一次', '重迭', '相同', '心意'],\n",
       " ['再度', '了解', '彼此'],\n",
       " ['相遇'],\n",
       " ['理由'],\n",
       " ['不', '知道', '是不是', '命运'],\n",
       " ['份', '喜悦', '不会', '改变', '喔'],\n",
       " ['某天', '放弃'],\n",
       " ['会', '说出', '次', '喜欢'],\n",
       " ['感谢', '能身', '这件', '事'],\n",
       " ['活着', '这件', '事', '感谢'],\n",
       " ['巨大', '跳动', '声', '传达'],\n",
       " ['重叠', '声响', '流泄', '思念'],\n",
       " ['约定', '一直', '相爱', '下去'],\n",
       " ['心跳', '停止']]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_list_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:28:47.898387Z",
     "start_time": "2020-05-15T06:28:47.891405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['heart', 'stop'],\n",
       " ['sure', 'world', 'think', 'fulli', 'enjoy'],\n",
       " ['leav', 'behind', 'pretti', 'much', 'noth'],\n",
       " ['side', 'think', 'want', 'keep', 'smile'],\n",
       " ['pound', 'chest', 'still', 'want', 'protect'],\n",
       " ['reason', 'live', 'fine'],\n",
       " ['one', 'one', 'count', 'tear'],\n",
       " ['know'],\n",
       " ['throb', 'puls', 'convey'],\n",
       " ['recur', 'sound', 'run', 'thought'],\n",
       " ['let', 'us', 'promis', 'apart', 'longer'],\n",
       " ['never', 'lone'],\n",
       " ['heart', 'one', 'minut'],\n",
       " ['seventi', 'time', 'shout', 'live'],\n",
       " ['run', 'fast'],\n",
       " ['one', 'hundr', 'ten', 'time', 'shout', 'love'],\n",
       " ['pound', 'chest', 'still', 'want', 'protect'],\n",
       " ['reason', 'live', 'fine'],\n",
       " ['heart', 'repeat'],\n",
       " ['know'],\n",
       " ['meet'],\n",
       " ['reason'],\n",
       " ['know', 'would', 'fate'],\n",
       " ['sheer', 'happi', 'unchang'],\n",
       " ['someday', 'end'],\n",
       " ['mani', 'love', 'utter'],\n",
       " ['offer', 'gratitud'],\n",
       " ['simpli', 'aliv', 'thank'],\n",
       " ['throb', 'puls', 'convey'],\n",
       " ['recur', 'sound', 'run', 'thought'],\n",
       " ['let', 'us', 'promis', 'keep', 'love'],\n",
       " ['heartbeat', 'stop']]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_list_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:41:20.859517Z",
     "start_time": "2020-05-15T06:41:20.810793Z"
    }
   },
   "outputs": [],
   "source": [
    "en_model = Word2Vec(en_list_proc, min_count=1, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:41:22.859562Z",
     "start_time": "2020-05-15T06:41:22.850587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keep', 0.9999463558197021),\n",
       " ('protect', 0.9996652007102966),\n",
       " ('leav', 0.9882749319076538),\n",
       " ('simpli', 0.9769353866577148),\n",
       " ('know', 0.9744161367416382),\n",
       " ('tear', 0.9601825475692749),\n",
       " ('longer', 0.9531025886535645),\n",
       " ('lone', 0.9482792019844055),\n",
       " ('chest', 0.9168407917022705),\n",
       " ('think', 0.8970745801925659)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.wv.most_similar(positive=[\"heart\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:41:30.067607Z",
     "start_time": "2020-05-15T06:41:30.013469Z"
    }
   },
   "outputs": [],
   "source": [
    "zh_model = Word2Vec(zh_list_proc, min_count=1, size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:41:30.405066Z",
     "start_time": "2020-05-15T06:41:30.398512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('觉得', 0.9999840259552002),\n",
       " ('再度', 0.9999455213546753),\n",
       " ('出', 0.9998087882995605),\n",
       " ('放弃', 0.9988545775413513),\n",
       " ('一个', 0.9917483329772949),\n",
       " ('做', 0.9864947199821472),\n",
       " ('思念', 0.9840846061706543),\n",
       " ('寂寞', 0.9760681390762329),\n",
       " ('说出', 0.9745391607284546),\n",
       " ('一次', 0.9745294451713562)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_model.wv.most_similar(positive=[\"心脏\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T03:45:48.183285Z",
     "start_time": "2020-05-14T03:45:48.178192Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Feat: No. of counts a word appears in the document\n",
    "# Eg. appear once in sentence, appear once in aligned sentence of other language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:41:42.494623Z",
     "start_time": "2020-05-15T06:41:42.489671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39296654"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.wv.similarity('heartbeat', 'heart') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:58:56.978298Z",
     "start_time": "2020-05-15T06:58:56.972863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13860422, -0.20425732], dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.wv['heart']+en_model.wv['stop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:53:46.746180Z",
     "start_time": "2020-05-15T06:53:46.740233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16470802, 0.07603731], dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_model.wv['心脏']+zh_model.wv['停下']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T07:00:37.123806Z",
     "start_time": "2020-05-15T07:00:37.118820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heart', 'stop']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_list_proc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T09:49:17.926797Z",
     "start_time": "2020-05-15T09:49:17.898518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: 0 \tSim: 0.16 \tPair: NO\n",
      "Line: 1 \tSim: -0.96 \tPair: NO\n",
      "Line: 2 \tSim: 0.27 \tPair: NO\n",
      "Line: 3 \tSim: -0.27 \tPair: NO\n",
      "Line: 4 \tSim: 0.98 \tPair: YES\n",
      "Line: 5 \tSim: 0.98 \tPair: YES\n",
      "Line: 6 \tSim: -0.42 \tPair: NO\n",
      "Line: 7 \tSim: -0.05 \tPair: NO\n",
      "Line: 8 \tSim: 0.57 \tPair: YES\n",
      "Line: 9 \tSim: 0.48 \tPair: NO\n",
      "Line: 10 \tSim: -0.33 \tPair: NO\n",
      "Line: 11 \tSim: -0.68 \tPair: NO\n",
      "Line: 12 \tSim: -0.21 \tPair: NO\n",
      "Line: 13 \tSim: 0.97 \tPair: YES\n",
      "Line: 14 \tSim: 0.99 \tPair: YES\n",
      "Line: 15 \tSim: 0.22 \tPair: NO\n",
      "Line: 16 \tSim: 0.98 \tPair: YES\n",
      "Line: 17 \tSim: 0.98 \tPair: YES\n",
      "Line: 18 \tSim: -0.16 \tPair: NO\n",
      "Line: 19 \tSim: -0.05 \tPair: NO\n",
      "Line: 20 \tSim: 0.99 \tPair: YES\n",
      "Line: 21 \tSim: 0.82 \tPair: YES\n",
      "Line: 22 \tSim: -0.9 \tPair: NO\n",
      "Line: 23 \tSim: -1.0 \tPair: NO\n",
      "Line: 24 \tSim: -0.99 \tPair: NO\n",
      "Line: 25 \tSim: -0.29 \tPair: NO\n",
      "Line: 26 \tSim: -0.99 \tPair: NO\n",
      "Line: 27 \tSim: -0.98 \tPair: NO\n",
      "Line: 28 \tSim: 0.57 \tPair: YES\n",
      "Line: 29 \tSim: 0.48 \tPair: NO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 0.9901083, 'Success Rate:33.3%')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_vector(model,sentence_list):\n",
    "    i=0\n",
    "    for word in sentence_list:\n",
    "        if i==0:\n",
    "            vector = model.wv[word]\n",
    "        else:\n",
    "            vector=vector+model.wv[word]\n",
    "        i+=1\n",
    "    return vector\n",
    "\n",
    "def cosine_similarity(v1,v2):\n",
    "    '''cosine_similarity(transformed_docs[2], transformed_docs[2])'''\n",
    "    ## Idk why need to np.squeeze (1,148) into (148,) shape to dot product [error: shapes not aligned]\n",
    "    ## toarray() [error: dimension mismatch]\n",
    "    \n",
    "    #print('Calculating Cosine Similarity...')\n",
    "    \n",
    "    #v1 = np.squeeze(v1.toarray())\n",
    "    #v2 = np.squeeze(v2.toarray())\n",
    "    return np.dot(v1,v2) / ( np.sqrt(np.dot(v1,v1)) * np.sqrt(np.dot(v2,v2)) )\n",
    "\n",
    "        \n",
    "def oof(n):\n",
    "    k=0\n",
    "    j=''\n",
    "    for i in range(n):\n",
    "        this_value = cosine_similarity( sum_vector(en_model,en_list_proc[i]), sum_vector(zh_model,zh_list_proc[i]) )\n",
    "        if i == 0:\n",
    "            most = this_value\n",
    "            s_index = 0\n",
    "        elif most <= this_value:\n",
    "            most = this_value\n",
    "            s_index = i\n",
    "        \n",
    "        if this_value>0.5:\n",
    "            j='YES'\n",
    "            k+=1\n",
    "        else:\n",
    "            j='NO'\n",
    "        \n",
    "        print('Line:',i,'\\tSim:',round(this_value,2), '\\tPair:', j)\n",
    "        \n",
    "    return s_index,most, 'Success Rate:'+str(round(k/n*100,1))+'%'\n",
    "            \n",
    "\n",
    "    \n",
    "oof(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:59:33.471625Z",
     "start_time": "2020-05-15T06:59:33.466681Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-15T06:59:44.010048Z",
     "start_time": "2020-05-15T06:59:44.004063Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
